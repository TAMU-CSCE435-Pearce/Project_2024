{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/scratch/group/csce435-f24/python-3.10.4/lib/python3.10/site-packages\")\n",
    "sys.path.append(\"/scratch/group/csce435-f24/thicket\")\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import thicket as th\n",
    "\n",
    "# Ensure pandas displays all rows/columns for easier inspection\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(1/2) Reading Files: 100%|██████████| 254/254 [00:05<00:00, 42.56it/s]\n",
      "(2/2) Creating Thicket: 100%|██████████| 253/253 [00:07<00:00, 34.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the performance data into a Thicket dataframe\n",
    "cali_files = glob(\"cali_outputs/*.cali\")  # Adjust the path to where your cali files are located\n",
    "tk = th.Thicket.from_caliperreader(cali_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.metadata_column_to_perfdata(\"input_size\")\n",
    "tk.metadata_column_to_perfdata(\"num_procs\")\n",
    "tk.metadata_column_to_perfdata(\"data_type\")\n",
    "\n",
    "# tk.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create the directory to save plots if it does not exist\n",
    "# output_dir = \"plots\"\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# # Define input sizes and thread counts (number of processes)\n",
    "# n_vals = [65536, 262144, 1048576, 4194304, 67108864]\n",
    "# thread_counts = [2, 4, 8, 16, 32]  # Replace with actual numbers of threads/ranks as needed\n",
    "# legends = [\"2^16\", \"2^18\", \"2^20\", \"2^22\", \"2^24\"]  # Legend labels for input sizes\n",
    "# data_types = ['s', 'r', 'v', 'p']  # List of data types to filter\n",
    "\n",
    "# # Loop through each data type\n",
    "# for data_type in data_types:\n",
    "#     plt.figure()  # Create a new figure for each data type\n",
    "#     plt.title(f\"Speedup for Data Type: {data_type}\")  # Set the title for the plot\n",
    "\n",
    "#     # Loop through each input size to calculate and plot speedup\n",
    "#     for n_val, label in zip(n_vals, legends):\n",
    "#         # Filter the dataframe by input size and data type\n",
    "#         df_filtered = tk.dataframe[(tk.dataframe['input_size'] == n_val) & (tk.dataframe['data_type'] == data_type)]\n",
    "\n",
    "#         # Extract baseline time for 2 threads/processes\n",
    "#         baseline_rows = df_filtered[df_filtered['num_procs'] == 2]\n",
    "#         if baseline_rows.empty:\n",
    "#             print(f\"No baseline data available for input size {n_val} with 2 processors and data_type {data_type}.\")\n",
    "#             continue\n",
    "\n",
    "#         baseline_time = baseline_rows['Avg time/rank'].values[0]\n",
    "\n",
    "#         # Calculate speedup for each thread count\n",
    "#         speedup_values = []\n",
    "#         for threads in thread_counts:\n",
    "#             # Check if data for this number of threads exists\n",
    "#             thread_row = df_filtered[df_filtered['num_procs'] == threads]\n",
    "#             if not thread_row.empty:\n",
    "#                 time_for_threads = thread_row['Avg time/rank'].values[0]\n",
    "#                 speedup = 2 * baseline_time / time_for_threads\n",
    "#                 speedup_values.append(speedup)\n",
    "#             else:\n",
    "#                 speedup_values.append(None)  # Placeholder for missing data\n",
    "#                 print(f\"No data available for input size {n_val} with {threads} processors and data_type {data_type}.\")\n",
    "\n",
    "#         # Plot speedup for current input size\n",
    "#         plt.plot(thread_counts, speedup_values, label=label)\n",
    "\n",
    "#     # Configure plot settings\n",
    "#     plt.xscale(\"log\", base=2)  # Logarithmic scale for thread counts\n",
    "#     plt.xlabel(\"Threads\")\n",
    "#     plt.ylabel(\"Speedup\")\n",
    "#     plt.legend(title=\"Input Size\")\n",
    "#     plt.grid(True)  # Optional: add grid for better visibility\n",
    "\n",
    "#     # Save the plot as an image file in the specified output directory\n",
    "# #     filename = f\"{output_dir}/speedup_data_type_{data_type}.png\"\n",
    "# #     plt.savefig(filename)\n",
    "# #     print(f\"Plot saved as {filename}\")\n",
    "\n",
    "#     # Display the plot for the current data type\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View Calltree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data available for size 16777216, input type 's', and threads 1024\n",
      "Plot saved to weak_scaling_plots/weak_scaling_sorted.png\n",
      "Plot saved to weak_scaling_plots/weak_scaling_random.png\n",
      "No data available for size 16777216, input type 'v', and threads 1024\n",
      "Plot saved to weak_scaling_plots/weak_scaling_reverse_sorted.png\n",
      "No data available for size 16777216, input type 'p', and threads 1024\n",
      "Plot saved to weak_scaling_plots/weak_scaling_1%_perturbed.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "output_dir = \"weak_scaling_plots\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# MPI Only\n",
    "# For Weak Scaling\n",
    "\n",
    "def calculate_weak_scaling(tk, input_type, initial_size=65536, max_threads=1024):\n",
    "    times = []\n",
    "    l = []\n",
    "\n",
    "    # List of starting thread counts\n",
    "    starting_threads = [2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "\n",
    "    # Loop through each starting thread count\n",
    "    for threads in starting_threads:\n",
    "        time = []\n",
    "        thread_l = []\n",
    "\n",
    "        size = initial_size\n",
    "        current_threads = threads\n",
    "\n",
    "        # Filter the dataframe for the required initial size and input type\n",
    "        base_df = tk.dataframe[(tk.dataframe['input_size'] == initial_size) &\n",
    "                               (tk.dataframe['data_type'] == input_type) &\n",
    "                               (tk.dataframe['num_procs'] == threads)]\n",
    "\n",
    "        # Check if there is data for the current configuration\n",
    "        if base_df.empty:\n",
    "            print(f\"No base time available for initial size {initial_size}, input type '{input_type}', and threads {threads}\")\n",
    "            continue\n",
    "\n",
    "        # Get the base time for the current starting number of threads\n",
    "        base = base_df['Avg time/rank'].values[0]\n",
    "\n",
    "        # Loop through each scaling configuration\n",
    "        while current_threads <= max_threads:\n",
    "            # Filter for the current size and threads\n",
    "            current_df = tk.dataframe[(tk.dataframe['input_size'] == size) &\n",
    "                                      (tk.dataframe['data_type'] == input_type) &\n",
    "                                      (tk.dataframe['num_procs'] == current_threads)]\n",
    "\n",
    "            # Check if there is data for the current configuration\n",
    "            if current_df.empty:\n",
    "                print(f\"No data available for size {size}, input type '{input_type}', and threads {current_threads}\")\n",
    "            else:\n",
    "                # Calculate the speedup\n",
    "                avg_time = current_df['Avg time/rank'].values[0]\n",
    "                speedup = base / avg_time\n",
    "                time.append(speedup)\n",
    "                thread_l.append(current_threads)\n",
    "\n",
    "            # Update the number of threads and input size\n",
    "            current_threads *= 4\n",
    "            size *= 4\n",
    "\n",
    "        # Append results if any data was found\n",
    "        if time:\n",
    "            times.append(time)\n",
    "            l.append(thread_l)\n",
    "\n",
    "    return l, times\n",
    "\n",
    "# Iterate through each input type and generate plots\n",
    "input_types = ['s', 'r', 'v', 'p']\n",
    "input_type_labels = {\n",
    "    's': 'Sorted',\n",
    "    'r': 'Random',\n",
    "    'v': 'Reverse Sorted',\n",
    "    'p': '1% Perturbed'\n",
    "}\n",
    "\n",
    "for input_type in input_types:\n",
    "    l, times = calculate_weak_scaling(tk, input_type)\n",
    "\n",
    "    # Plot the results if any data was generated\n",
    "    if times:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for i in range(len(times)):\n",
    "            plt.plot(l[i], times[i], marker='o')\n",
    "\n",
    "        plt.legend([f\"2^{int(np.log2(65536 * (4**i)))} e/t\" for i in range(len(times))])\n",
    "        plt.xscale(\"log\", base=2)\n",
    "        plt.xlabel(\"Threads\")\n",
    "        plt.ylabel(\"Speedup (Normalized Time per Rank)\")\n",
    "        plt.title(f\"Weak Scaling for Input Type: {input_type_labels[input_type]}\")\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Save the plot as a PNG file\n",
    "        plot_filename = f\"weak_scaling_{input_type_labels[input_type].replace(' ', '_').lower()}.png\"\n",
    "        plot_path = os.path.join(output_dir, plot_filename)\n",
    "        plt.savefig(plot_path, format='png')\n",
    "        plt.close()\n",
    "        print(f\"Plot saved to {plot_path}\")\n",
    "    else:\n",
    "        print(f\"No data available to plot for input type '{input_type}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def export_process_time_csv(df, input_size=67108864, input_type=\"random\", output_file=\"process_time_data.csv\"):\n",
    "#     \"\"\"\n",
    "#     Exports the number of processes vs. time for a specified input size and input type to a CSV file.\n",
    "\n",
    "#     Parameters:\n",
    "#     - df: DataFrame containing Caliper data with 'input_size', 'input_type', 'num_procs', and 'Avg time/rank' columns.\n",
    "#     - input_size: Specific input size to filter (default is 2^16).\n",
    "#     - input_type: Specific input type to filter (default is 'random').\n",
    "#     - output_file: Name of the output CSV file (default is 'process_time_data.csv').\n",
    "#     \"\"\"\n",
    "#     # Filter the data based on input size and input type\n",
    "#     filtered_df = df[(df['input_size'] == input_size) &\n",
    "#                      (df['data_type'] == data_type) &\n",
    "#                      (df['name'] == \"MPI_Comm_dup\")]\n",
    "\n",
    "#     print(filtered_df)\n",
    "    \n",
    "#     # Check if there is any data available after filtering\n",
    "#     if filtered_df.empty:\n",
    "#         print(f\"No data available for input size {input_size} and input type {input_type}.\")\n",
    "#         return\n",
    "    \n",
    "#     # Select only the columns for number of processes and average time per rank\n",
    "#     process_time_df = filtered_df[['num_procs', 'Total time']].sort_values(by='num_procs')\n",
    "\n",
    "\n",
    "\n",
    "#     # Save the result to a CSV file\n",
    "#     process_time_df.to_csv(output_file, index=False)\n",
    "#     print(f\"Data successfully saved to {output_file}\")\n",
    "\n",
    "# # Example usage with Caliper data DataFrame (tk.dataframe):\n",
    "# export_process_time_csv(df=tk.dataframe, input_size=67108864, input_type=\"random\", output_file=\"merge_sort_process_time.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def export_speedup_data_csv(df, input_size=67108864, input_type=\"random\", mpi_function=\"MPI_Comm_dup\", output_file=\"speedup_data.csv\"):\n",
    "#     \"\"\"\n",
    "#     Exports the speedup data for a specified input size, input type, and MPI function to a CSV file.\n",
    "\n",
    "#     Parameters:\n",
    "#     - df: DataFrame containing Caliper data with 'input_size', 'input_type', 'num_procs', 'Total time', and 'name' columns.\n",
    "#     - input_size: Specific input size to filter (default is 2^16).\n",
    "#     - input_type: Specific input type to filter (default is 'random').\n",
    "#     - mpi_function: Name of the MPI function to filter (default is 'MPI_Finalized').\n",
    "#     - output_file: Name of the output CSV file (default is 'speedup_data.csv').\n",
    "#     \"\"\"\n",
    "#     # Filter the data based on input size, input type, and MPI function\n",
    "#     filtered_df = df[(df['input_size'] == input_size) &\n",
    "#                      (df['data_type'] == data_type) &\n",
    "#                      (df['name'] == mpi_function)]\n",
    "        \n",
    "#     print(filtered_df)\n",
    "    \n",
    "#     # Check if there is any data available after filtering\n",
    "#     if filtered_df.empty:\n",
    "#         print(f\"No data available for input size {input_size}, input type {input_type}, and MPI function {mpi_function}.\")\n",
    "#         return\n",
    "    \n",
    "#     # Select only the relevant columns\n",
    "#     process_total_time_df = filtered_df[['num_procs', 'Total time']]\n",
    "    \n",
    "#     # Calculate speedup based on the baseline time (using the minimum number of processes)\n",
    "#     baseline_time = process_total_time_df['Total time'].min()  # Get the baseline time\n",
    "#     process_total_time_df['Speedup'] = baseline_time / process_total_time_df['Total time']\n",
    "    \n",
    "#     # Sort by number of processes\n",
    "#     process_total_time_df = process_total_time_df.sort_values(by='num_procs')\n",
    "\n",
    "#     # Save the result to a CSV file\n",
    "#     process_total_time_df.to_csv(output_file, index=False)\n",
    "#     print(f\"Speedup data successfully saved to {output_file}.\")\n",
    "\n",
    "# # Example usage with Caliper data DataFrame (tk.dataframe):\n",
    "# export_speedup_data_csv(df=tk.dataframe, input_size=67108864, input_type=\"random\", output_file=\"merge_sort_speedup_data26.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_strong_scaling(df, input_size, title):\n",
    "#     os.makedirs('figures', exist_ok=True)\n",
    "\n",
    "#     plt.figure(figsize=(15, 7), facecolor='white')\n",
    "#     export_data = []\n",
    "\n",
    "    \n",
    "#     for data_type in df.index.ge t_level_values('data_type').unique():\n",
    "#         subset = df.xs((input_size, data_type), level=('input_size', 'data_type'), drop_level=False)\n",
    "# #         print(subset)\n",
    "\n",
    "#         # Filter out non-positive values for plotting\n",
    "#         subset = subset[subset['Avg time/rank'] > 0]\n",
    "        \n",
    "#          # Append data for CSV, including \"sample_sort\" and data_type\n",
    "#         for num_procs, avg_time in zip(subset.index.get_level_values('num_procs'), subset['Avg time/rank']):\n",
    "#             export_data.append([num_procs, avg_time, \"sample_sort\", data_type])\n",
    "\n",
    "#     # Export data to CSV file with data_type column\n",
    "#     csv_path = f\"figures/{title}_{input_size}_strong_scaling_data.csv\"\n",
    "#     export_df = pd.DataFrame(export_data, columns=['Number of Processes', 'Avg Time per Rank (seconds)', 'Algorithm', 'Data Type'])\n",
    "#     export_df.to_csv(csv_path, index=False)\n",
    "    \n",
    "#     if pd.notna(input_size):\n",
    "#         input_size_str = f\"$2^{{{int(np.log2(input_size))}}}$\"\n",
    "#     else:\n",
    "#         input_size_str = \"Unknown Size\"\n",
    "\n",
    "#     plt.title(f\"{title}: Strong Scaling (Input Size: {input_size_str})\")\n",
    "#     plt.xlabel('Number of Processes')\n",
    "#     plt.ylabel('Avg Time per Rank (seconds)')\n",
    "#     plt.xscale('log', base=2)\n",
    "#     plt.yscale('log')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     save_path = f\"figures/{title}_{input_size_str}_strong_scaling.png\"\n",
    "#     plt.savefig(save_path, format='png', bbox_inches='tight')\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
